{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3D without transfer leanring\n",
    "this module run C3D. We will train the model by using the data provided by Jake without using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load dataset and preprocess\n",
    " - First, run DataProprocessing Notebook to get label (Y) and data (X) txt file. \n",
    " - Second, move label and data txt file into this notebook foler\n",
    " - third, run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (81, 5, 64, 64, 3)\n",
      "Train labels shape:  (81,)\n",
      "Validation data shape:  (10, 5, 64, 64, 3)\n",
      "Validation labels shape:  (10,)\n",
      "Test data shape:  (10, 5, 64, 64, 3)\n",
      "Test labels shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "def get_data(num_training = 81, num_validation = 10, num_test = 10):\n",
    "    ''' \n",
    "    load the training data provided by Jake \n",
    "    \n",
    "    '''\n",
    "    # load the raw data\n",
    "    with open('label.json', 'r') as fp:\n",
    "        y_total = json.load(fp)\n",
    "        y_total = np.array(y_total)\n",
    "    with open('data.json', 'r') as fp:\n",
    "        x_total = json.load(fp)\n",
    "        x_total = np.array(x_total)\n",
    "        \n",
    "    # Subsample the data\n",
    "    # training data\n",
    "    mask_train = range(0, num_training)\n",
    "    x_train = x_total[mask_train]\n",
    "    y_train = y_total[mask_train]\n",
    "    # validation data\n",
    "    mask_val = range(num_training, num_training + num_validation)\n",
    "    x_val = x_total[mask_val]\n",
    "    y_val = y_total[mask_val]\n",
    "    # test data\n",
    "    mask_test = range(num_training+num_validation, num_training+num_validation+num_test)\n",
    "    x_test = x_total[mask_test]\n",
    "    y_test = y_total[mask_test]\n",
    "    \n",
    "    # normalize the data: subtract the mean\n",
    "    mean_video = np.floor(np.mean(x_train, axis=0))\n",
    "    x_train = x_train - mean_video\n",
    "    x_val = x_val - mean_video\n",
    "    x_test = x_test - mean_video\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data()\n",
    "\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', x_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful utilities\n",
    "Remember that our image data is initially N x F x H x W x C, where:\n",
    " - N is the number of datapoints\n",
    " - F is the frame of video\n",
    " - H is the height of each frame in pixels\n",
    " - W is the height of each frame in pixels\n",
    " - C is the number of channels (usually 3: R, G, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C3D model\n",
    "train the model by using C3D. As for this case, we will have two layer nerual network. The first layer will be a 3D convolutional layer and the second layer will be a FC layer.\n",
    "- The input for C3D should look like N x F x H x W x C. [batch, in_depth, in_height, in_width, in_channels].\n",
    "- The filter for C3D should look like D x H x W x I_C x I_C. [filter_depth, filter_height, filter_width, in_channels, out_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 5, 64, 64, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "reg = 0.1\n",
    "\n",
    "# define our model\n",
    "def simple_model(X,y):\n",
    "    regularizers = 0\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[3, 3, 3, 3, 4])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[4])\n",
    "    regularizers += tf.nn.l2_loss(Wconv1)\n",
    "    # (64 - 3 + 0)/2 + 1 = 31\n",
    "    # (5 - 3 + 0)/2 + 1 = 2\n",
    "    # 7688 = 2*31*31*4. with no padding and out dim is \n",
    "    W1 = tf.get_variable(\"W1\", shape=[7688, 4]) \n",
    "    b1 = tf.get_variable(\"b1\", shape=[4])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    # valid padding means no padding\n",
    "    a1 = tf.nn.conv3d(X, Wconv1, strides=[1,2,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,7688]) # -1 is N \n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out, regularizers\n",
    "\n",
    "y_out, regularizers = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,4),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss+regularizers*reg)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=40, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 12 and accuracy of 0.45\n",
      "Epoch 1, Overall loss = 9.57 and accuracy of 0.432\n",
      "Epoch 2, Overall loss = 7 and accuracy of 0.457\n",
      "Epoch 3, Overall loss = 6.45 and accuracy of 0.383\n",
      "Iteration 10: with minibatch training loss = 4.24 and accuracy of 0.62\n",
      "Epoch 4, Overall loss = 3.88 and accuracy of 0.63\n",
      "Epoch 5, Overall loss = 3.83 and accuracy of 0.568\n",
      "Epoch 6, Overall loss = 2.34 and accuracy of 0.765\n",
      "Iteration 20: with minibatch training loss = 0.0168 and accuracy of 1\n",
      "Epoch 7, Overall loss = 1.5 and accuracy of 0.765\n",
      "Epoch 8, Overall loss = 1.6 and accuracy of 0.827\n",
      "Epoch 9, Overall loss = 0.943 and accuracy of 0.889\n",
      "Epoch 10, Overall loss = 0.548 and accuracy of 0.901\n",
      "Iteration 30: with minibatch training loss = 0.536 and accuracy of 0.88\n",
      "Epoch 11, Overall loss = 0.411 and accuracy of 0.914\n",
      "Epoch 12, Overall loss = 0.412 and accuracy of 0.951\n",
      "Epoch 13, Overall loss = 0.234 and accuracy of 0.975\n",
      "Iteration 40: with minibatch training loss = 0.129 and accuracy of 0.97\n",
      "Epoch 14, Overall loss = 0.154 and accuracy of 0.975\n",
      "Epoch 15, Overall loss = 0.0846 and accuracy of 0.963\n",
      "Epoch 16, Overall loss = 0.0625 and accuracy of 0.988\n",
      "Iteration 50: with minibatch training loss = 0.0167 and accuracy of 1\n",
      "Epoch 17, Overall loss = 0.0426 and accuracy of 0.988\n",
      "Epoch 18, Overall loss = 0.0248 and accuracy of 0.988\n",
      "Epoch 19, Overall loss = 0.035 and accuracy of 0.988\n",
      "Epoch 20, Overall loss = 0.0219 and accuracy of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.021871793394287426, 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,x_train,y_train,20,40,10,train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 0.0167 and accuracy of 1\n",
      "Validation\n",
      "Epoch 1, Overall loss = 3.9 and accuracy of 0.7\n",
      "Test\n",
      "Epoch 1, Overall loss = 3.36 and accuracy of 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.3620188236236572, 0.5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,x_train,y_train,1,40)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,x_val,y_val,1,10)\n",
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,x_test,y_test,1,10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowroot]",
   "language": "python",
   "name": "conda-env-tensorflowroot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
