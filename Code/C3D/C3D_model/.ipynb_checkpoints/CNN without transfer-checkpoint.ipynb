{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN without transfer\n",
    "this module run CNN without using C3D. We will train the model by using the data provided by Jake without using transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load dataset and preprocess\n",
    " - First, run DataProprocessing Notebook to get label (Y) and data (X) txt file. \n",
    " - Second, move label and data txt file into this notebook foler\n",
    " - third, run the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (81, 5, 64, 64, 3)\n",
      "Train labels shape:  (81,)\n",
      "Validation data shape:  (10, 5, 64, 64, 3)\n",
      "Validation labels shape:  (10,)\n",
      "Test data shape:  (10, 5, 64, 64, 3)\n",
      "Test labels shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "def get_data(num_training = 81, num_validation = 10, num_test = 10):\n",
    "    ''' \n",
    "    load the training data provided by Jake \n",
    "    \n",
    "    '''\n",
    "    # load the raw data\n",
    "    with open('label.json', 'r') as fp:\n",
    "        y_total = json.load(fp)\n",
    "        y_total = np.array(y_total)\n",
    "    with open('data.json', 'r') as fp:\n",
    "        x_total = json.load(fp)\n",
    "        x_total = np.array(x_total)\n",
    "        \n",
    "    # Subsample the data\n",
    "    # training data\n",
    "    mask_train = range(0, num_training)\n",
    "    x_train = x_total[mask_train]\n",
    "    y_train = y_total[mask_train]\n",
    "    # validation data\n",
    "    mask_val = range(num_training, num_training + num_validation)\n",
    "    x_val = x_total[mask_val]\n",
    "    y_val = y_total[mask_val]\n",
    "    # test data\n",
    "    mask_test = range(num_training+num_validation, num_training+num_validation+num_test)\n",
    "    x_test = x_total[mask_test]\n",
    "    y_test = y_total[mask_test]\n",
    "    \n",
    "    # normalize the data: subtract the mean\n",
    "    mean_video = np.floor(np.mean(x_train, axis=0))\n",
    "    x_train = x_train - mean_video\n",
    "    x_val = x_val - mean_video\n",
    "    x_test = x_test - mean_video\n",
    "    \n",
    "    return x_train, y_train, x_val, y_val, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = get_data()\n",
    "\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', x_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful utilities\n",
    "Remember that our image data is initially N x F x H x W x C, where:\n",
    " - N is the number of datapoints\n",
    " - F is the frame of video\n",
    " - H is the height of each frame in pixels\n",
    " - W is the height of each frame in pixels\n",
    " - C is the number of channels (usually 3: R, G, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy model\n",
    "train a dummy model by simpily convert the data x shape to N x H x W x F, while using RGB to average all three channels\n",
    "This dummy model has two layer with the first one to be a convolutional layer and the second one to be a FC layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (81, 64, 64, 5)\n",
      "Train labels shape:  (81,)\n",
      "Validation data shape:  (10, 64, 64, 5)\n",
      "Validation labels shape:  (10,)\n",
      "Test data shape:  (10, 64, 64, 5)\n",
      "Test labels shape:  (10,)\n"
     ]
    }
   ],
   "source": [
    "x_train = 0.21 * x_train[:,:,:,:,0] + 0.72 * x_train[:,:,:,:,1] + 0.07 * x_train[:,:,:,:,2]\n",
    "x_val = 0.21 * x_val[:,:,:,:,0] + 0.72 * x_val[:,:,:,:,1] + 0.07 * x_val[:,:,:,:,2]\n",
    "x_test = 0.21 * x_test[:,:,:,:,0] + 0.72 * x_test[:,:,:,:,1] + 0.07 * x_test[:,:,:,:,2]\n",
    "\n",
    "N, F, H, W = x_train.shape\n",
    "x_train = np.reshape(x_train, (N,H,W,F))\n",
    "N, F, H, W = x_val.shape\n",
    "x_val = np.reshape(x_val, (N,H,W,F))\n",
    "N, F, H, W = x_test.shape\n",
    "x_test = np.reshape(x_test, (N,H,W,F))\n",
    "print('Train data shape: ', x_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', x_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', x_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, 64, 64, 5])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "# define our model\n",
    "def simple_model(X,y):\n",
    "    # define our weights (e.g. init_two_layer_convnet)\n",
    "    \n",
    "    # setup variables\n",
    "    Wconv1 = tf.get_variable(\"Wconv1\", shape=[7, 7, 5, 16])\n",
    "    bconv1 = tf.get_variable(\"bconv1\", shape=[16])\n",
    "    # 13456 = 32*29*29. with no padding and out dim is (64 - 7 + 0)/2 + 1 = 29\n",
    "    W1 = tf.get_variable(\"W1\", shape=[13456, 4]) \n",
    "    b1 = tf.get_variable(\"b1\", shape=[4])\n",
    "\n",
    "    # define our graph (e.g. two_layer_convnet)\n",
    "    # valid padding means no padding\n",
    "    a1 = tf.nn.conv2d(X, Wconv1, strides=[1,2,2,1], padding='VALID') + bconv1\n",
    "    h1 = tf.nn.relu(a1)\n",
    "    h1_flat = tf.reshape(h1,[-1,13456]) # -1 is N \n",
    "    y_out = tf.matmul(h1_flat,W1) + b1\n",
    "    return y_out\n",
    "\n",
    "y_out = simple_model(X,y)\n",
    "\n",
    "# define our loss\n",
    "total_loss = tf.losses.hinge_loss(tf.one_hot(y,4),logits=y_out)\n",
    "mean_loss = tf.reduce_mean(total_loss)\n",
    "\n",
    "# define our optimizer\n",
    "optimizer = tf.train.AdamOptimizer(5e-4) # select optimizer and set learning rate\n",
    "train_step = optimizer.minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs=1, batch_size=40, print_every=100,\n",
    "              training=None, plot_losses=False):\n",
    "    # have tensorflow compute accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(predict,1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(Xd.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "\n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss,correct_prediction,accuracy]\n",
    "    \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "    \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for e in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        # make sure we iterate over the dataset once\n",
    "        for i in range(int(math.ceil(Xd.shape[0]/batch_size))):\n",
    "            # generate indicies for the batch\n",
    "            start_idx = (i*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx],\n",
    "                         is_training: training_now }\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            # have tensorflow compute loss and correct predictions\n",
    "            # and (if given) perform a training step\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "            \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            # print every now and then\n",
    "            if training_now and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                      .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,e+1))\n",
    "        if plot_losses:\n",
    "            plt.plot(losses)\n",
    "            plt.grid(True)\n",
    "            plt.title('Epoch {} Loss'.format(e+1))\n",
    "            plt.xlabel('minibatch number')\n",
    "            plt.ylabel('minibatch loss')\n",
    "            plt.show()\n",
    "    return total_loss,total_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Iteration 0: with minibatch training loss = 6.79 and accuracy of 0.2\n",
      "Epoch 1, Overall loss = 13.7 and accuracy of 0.333\n",
      "Epoch 2, Overall loss = 8.14 and accuracy of 0.617\n",
      "Epoch 3, Overall loss = 6.46 and accuracy of 0.63\n",
      "Iteration 10: with minibatch training loss = 10.4 and accuracy of 0.68\n",
      "Epoch 4, Overall loss = 8.31 and accuracy of 0.531\n",
      "Epoch 5, Overall loss = 6.81 and accuracy of 0.704\n",
      "Epoch 6, Overall loss = 6.68 and accuracy of 0.815\n",
      "Iteration 20: with minibatch training loss = 0 and accuracy of 1\n",
      "Epoch 7, Overall loss = 5.8 and accuracy of 0.778\n",
      "Epoch 8, Overall loss = 5.47 and accuracy of 0.864\n",
      "Epoch 9, Overall loss = 4.76 and accuracy of 0.901\n",
      "Epoch 10, Overall loss = 3.88 and accuracy of 0.864\n",
      "Iteration 30: with minibatch training loss = 1.54 and accuracy of 0.93\n",
      "Epoch 11, Overall loss = 3.37 and accuracy of 0.914\n",
      "Epoch 12, Overall loss = 3.28 and accuracy of 0.901\n",
      "Epoch 13, Overall loss = 2.56 and accuracy of 0.926\n",
      "Iteration 40: with minibatch training loss = 3.35 and accuracy of 0.88\n",
      "Epoch 14, Overall loss = 1.94 and accuracy of 0.914\n",
      "Epoch 15, Overall loss = 1.73 and accuracy of 0.951\n",
      "Epoch 16, Overall loss = 1.52 and accuracy of 0.975\n",
      "Iteration 50: with minibatch training loss = 0 and accuracy of 1\n",
      "Epoch 17, Overall loss = 1.34 and accuracy of 0.988\n",
      "Epoch 18, Overall loss = 1.3 and accuracy of 0.975\n",
      "Epoch 19, Overall loss = 1.16 and accuracy of 0.988\n",
      "Epoch 20, Overall loss = 1.08 and accuracy of 0.988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.0761806460810297, 0.98765432098765427)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,x_train,y_train,20,40,10,train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch 1, Overall loss = 1 and accuracy of 0.988\n",
      "Validation\n",
      "Epoch 1, Overall loss = 3.68 and accuracy of 0.5\n",
      "Test\n",
      "Epoch 1, Overall loss = 13.7 and accuracy of 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13.742609024047852, 0.59999999999999998)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Training')\n",
    "run_model(sess,y_out,mean_loss,x_train,y_train,1,40)\n",
    "print('Validation')\n",
    "run_model(sess,y_out,mean_loss,x_val,y_val,1,10)\n",
    "print('Test')\n",
    "run_model(sess,y_out,mean_loss,x_test,y_test,1,10)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowroot]",
   "language": "python",
   "name": "conda-env-tensorflowroot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
