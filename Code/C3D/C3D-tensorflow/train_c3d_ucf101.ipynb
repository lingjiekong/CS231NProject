{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2015 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"Trains and Evaluates the MNIST network using a feed dictionary.\"\"\"\n",
    "# pylint: disable=missing-docstring\n",
    "import os\n",
    "import time\n",
    "import numpy\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import input_data\n",
    "import c3d_model\n",
    "import math\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only call `sparse_softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-21654622e258>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\kongl\\Anaconda2\\envs\\tensorflowroot\\lib\\site-packages\\tensorflow\\python\\platform\\app.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(main, argv)\u001b[0m\n\u001b[0;32m     46\u001b[0m   \u001b[1;31m# Call the main function, passing through any arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m   \u001b[1;31m# to the final program.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m   \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-21654622e258>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(_)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 243\u001b[1;33m   \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-21654622e258>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m    155\u001b[0m                           \u001b[0mscope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                           \u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                           \u001b[0mlabels_placeholder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgpu_index\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgpu_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m                           )\n\u001b[0;32m    159\u001b[0m           \u001b[0mgrads1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopt1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvarlist1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-21654622e258>\u001b[0m in \u001b[0;36mtower_loss\u001b[1;34m(name_scope, logit, labels)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtower_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m   cross_entropy_mean = tf.reduce_mean(\n\u001b[1;32m---> 51\u001b[1;33m                   \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m                   )\n\u001b[0;32m     53\u001b[0m   tf.scalar_summary(\n",
      "\u001b[1;32mC:\\Users\\kongl\\Anaconda2\\envs\\tensorflowroot\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[1;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[0;32m   1711\u001b[0m   \"\"\"\n\u001b[0;32m   1712\u001b[0m   _ensure_xent_args(\"sparse_softmax_cross_entropy_with_logits\", _sentinel,\n\u001b[1;32m-> 1713\u001b[1;33m                     labels, logits)\n\u001b[0m\u001b[0;32m   1714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1715\u001b[0m   \u001b[1;31m# TODO(pcmurray) Raise an error when the label is not an index in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kongl\\Anaconda2\\envs\\tensorflowroot\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m_ensure_xent_args\u001b[1;34m(name, sentinel, labels, logits)\u001b[0m\n\u001b[0;32m   1560\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0msentinel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m     raise ValueError(\"Only call `%s` with \"\n\u001b[1;32m-> 1562\u001b[1;33m                      \"named arguments (labels=..., logits=..., ...)\" % name)\n\u001b[0m\u001b[0;32m   1563\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlogits\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Both labels and logits must be provided.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Only call `sparse_softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)"
     ]
    }
   ],
   "source": [
    "# Basic model parameters as external flags.\n",
    "flags = tf.app.flags\n",
    "gpu_num = 1\n",
    "#flags.DEFINE_float('learning_rate', 0.0, 'Initial learning rate.')\n",
    "flags.DEFINE_integer('max_steps', 5000, 'Number of steps to run trainer.')\n",
    "flags.DEFINE_integer('batch_size', 10, 'Batch size.')\n",
    "FLAGS = flags.FLAGS\n",
    "MOVING_AVERAGE_DECAY = 0.9999\n",
    "model_save_dir = './models'\n",
    "\n",
    "def placeholder_inputs(batch_size):\n",
    "  \"\"\"Generate placeholder variables to represent the input tensors.\n",
    "\n",
    "  These placeholders are used as inputs by the rest of the model building\n",
    "  code and will be fed from the downloaded data in the .run() loop, below.\n",
    "\n",
    "  Args:\n",
    "    batch_size: The batch size will be baked into both placeholders.\n",
    "\n",
    "  Returns:\n",
    "    images_placeholder: Images placeholder.\n",
    "    labels_placeholder: Labels placeholder.\n",
    "  \"\"\"\n",
    "  # Note that the shapes of the placeholders match the shapes of the full\n",
    "  # image and label tensors, except the first dimension is now batch_size\n",
    "  # rather than the full size of the train or test data sets.\n",
    "  images_placeholder = tf.placeholder(tf.float32, shape=(batch_size,\n",
    "                                                         c3d_model.NUM_FRAMES_PER_CLIP,\n",
    "                                                         c3d_model.CROP_SIZE,\n",
    "                                                         c3d_model.CROP_SIZE,\n",
    "                                                         c3d_model.CHANNELS))\n",
    "  labels_placeholder = tf.placeholder(tf.int64, shape=(batch_size))\n",
    "  return images_placeholder, labels_placeholder\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "  average_grads = []\n",
    "  for grad_and_vars in zip(*tower_grads):\n",
    "    grads = []\n",
    "    for g, _ in grad_and_vars:\n",
    "      expanded_g = tf.expand_dims(g, 0)\n",
    "      grads.append(expanded_g)\n",
    "    grad = tf.concat(0, grads)\n",
    "    grad = tf.reduce_mean(grad, 0)\n",
    "    v = grad_and_vars[0][1]\n",
    "    grad_and_var = (grad, v)\n",
    "    average_grads.append(grad_and_var)\n",
    "  return average_grads\n",
    "\n",
    "def tower_loss(name_scope, logit, labels):\n",
    "  cross_entropy_mean = tf.reduce_mean(\n",
    "                  tf.nn.sparse_softmax_cross_entropy_with_logits(logit, labels)\n",
    "                  )\n",
    "  tf.scalar_summary(\n",
    "                  name_scope + 'cross entropy',\n",
    "                  cross_entropy_mean\n",
    "                  )\n",
    "  weight_decay_loss = tf.add_n(tf.get_collection('losses', name_scope))\n",
    "  tf.scalar_summary(name_scope + 'weight decay loss', weight_decay_loss)\n",
    "  tf.add_to_collection('losses', cross_entropy_mean)\n",
    "  losses = tf.get_collection('losses', name_scope)\n",
    "\n",
    "  # Calculate the total loss for the current tower.\n",
    "  total_loss = tf.add_n(losses, name='total_loss')\n",
    "  tf.scalar_summary(name_scope + 'total loss', total_loss)\n",
    "\n",
    "  # Compute the moving average of all individual losses and the total loss.\n",
    "  loss_averages = tf.train.ExponentialMovingAverage(0.99, name='loss')\n",
    "  loss_averages_op = loss_averages.apply(losses + [total_loss])\n",
    "  with tf.control_dependencies([loss_averages_op]):\n",
    "    total_loss = tf.identity(total_loss)\n",
    "  return total_loss\n",
    "\n",
    "def tower_acc(logit, labels):\n",
    "  correct_pred = tf.equal(tf.argmax(logit, 1), labels)\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "  return accuracy\n",
    "\n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  with tf.device('/cpu:0'):\n",
    "    var = tf.get_variable(name, shape, initializer=initializer)\n",
    "  return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, wd):\n",
    "  var = _variable_on_cpu(name, shape, tf.contrib.layers.xavier_initializer())\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var\n",
    "\n",
    "def run_training():\n",
    "  # Get the sets of images and labels for training, validation, and\n",
    "  # Tell TensorFlow that the model will be built into the default Graph.\n",
    "\n",
    "  # Create model directory\n",
    "  if not os.path.exists(model_save_dir):\n",
    "      os.makedirs(model_save_dir)\n",
    "  use_pretrained_model = False\n",
    "  model_filename = \"./sports1m_finetuning_ucf101.model\"\n",
    "\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.get_variable(\n",
    "                    'global_step',\n",
    "                    [],\n",
    "                    initializer=tf.constant_initializer(0),\n",
    "                    trainable=False\n",
    "                    )\n",
    "    images_placeholder, labels_placeholder = placeholder_inputs(\n",
    "                    FLAGS.batch_size * gpu_num\n",
    "                    )\n",
    "    tower_grads1 = []\n",
    "    tower_grads2 = []\n",
    "    logits = []\n",
    "    opt1 = tf.train.AdamOptimizer(1e-4)\n",
    "    opt2 = tf.train.AdamOptimizer(2e-4)\n",
    "    for gpu_index in range(0, gpu_num):\n",
    "      with tf.device('/gpu:%d' % gpu_index):\n",
    "        with tf.name_scope('%s_%d' % ('dextro-research', gpu_index)) as scope:\n",
    "          with tf.variable_scope('var_name') as var_scope:\n",
    "            weights = {\n",
    "              'wc1': _variable_with_weight_decay('wc1', [3, 3, 3, 3, 64], 0.0005),\n",
    "              'wc2': _variable_with_weight_decay('wc2', [3, 3, 3, 64, 128], 0.0005),\n",
    "              'wc3a': _variable_with_weight_decay('wc3a', [3, 3, 3, 128, 256], 0.0005),\n",
    "              'wc3b': _variable_with_weight_decay('wc3b', [3, 3, 3, 256, 256], 0.0005),\n",
    "              'wc4a': _variable_with_weight_decay('wc4a', [3, 3, 3, 256, 512], 0.0005),\n",
    "              'wc4b': _variable_with_weight_decay('wc4b', [3, 3, 3, 512, 512], 0.0005),\n",
    "              'wc5a': _variable_with_weight_decay('wc5a', [3, 3, 3, 512, 512], 0.0005),\n",
    "              'wc5b': _variable_with_weight_decay('wc5b', [3, 3, 3, 512, 512], 0.0005),\n",
    "              'wd1': _variable_with_weight_decay('wd1', [8192, 4096], 0.0005),\n",
    "              'wd2': _variable_with_weight_decay('wd2', [4096, 4096], 0.0005),\n",
    "              'out': _variable_with_weight_decay('wout', [4096, c3d_model.NUM_CLASSES], 0.0005)\n",
    "              }\n",
    "            biases = {\n",
    "              'bc1': _variable_with_weight_decay('bc1', [64], 0.000),\n",
    "              'bc2': _variable_with_weight_decay('bc2', [128], 0.000),\n",
    "              'bc3a': _variable_with_weight_decay('bc3a', [256], 0.000),\n",
    "              'bc3b': _variable_with_weight_decay('bc3b', [256], 0.000),\n",
    "              'bc4a': _variable_with_weight_decay('bc4a', [512], 0.000),\n",
    "              'bc4b': _variable_with_weight_decay('bc4b', [512], 0.000),\n",
    "              'bc5a': _variable_with_weight_decay('bc5a', [512], 0.000),\n",
    "              'bc5b': _variable_with_weight_decay('bc5b', [512], 0.000),\n",
    "              'bd1': _variable_with_weight_decay('bd1', [4096], 0.000),\n",
    "              'bd2': _variable_with_weight_decay('bd2', [4096], 0.000),\n",
    "              'out': _variable_with_weight_decay('bout', [c3d_model.NUM_CLASSES], 0.000),\n",
    "              }\n",
    "          varlist1 = weights.values()\n",
    "          varlist2 = biases.values()\n",
    "          logit = c3d_model.inference_c3d(\n",
    "                          images_placeholder[gpu_index * FLAGS.batch_size:(gpu_index + 1) * FLAGS.batch_size,:,:,:,:],\n",
    "                          0.5,\n",
    "                          FLAGS.batch_size,\n",
    "                          weights,\n",
    "                          biases\n",
    "                          )\n",
    "          loss = tower_loss(\n",
    "                          scope,\n",
    "                          logit,\n",
    "                          labels_placeholder[gpu_index * FLAGS.batch_size:(gpu_index + 1) * FLAGS.batch_size]\n",
    "                          )\n",
    "          grads1 = opt1.compute_gradients(loss, varlist1)\n",
    "          grads2 = opt2.compute_gradients(loss, varlist2)\n",
    "          tower_grads1.append(grads1)\n",
    "          tower_grads2.append(grads2)\n",
    "          logits.append(logit)\n",
    "          tf.get_variable_scope().reuse_variables()\n",
    "    logits = tf.concat(0, logits)\n",
    "    accuracy = tower_acc(logits, labels_placeholder)\n",
    "    tf.scalar_summary('accuracy', accuracy)\n",
    "    grads1 = average_gradients(tower_grads1)\n",
    "    grads2 = average_gradients(tower_grads2)\n",
    "    apply_gradient_op1 = opt1.apply_gradients(grads1)\n",
    "    apply_gradient_op2 = opt2.apply_gradients(grads2, global_step=global_step)\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "    train_op = tf.group(apply_gradient_op1, apply_gradient_op2, variables_averages_op)\n",
    "    null_op = tf.no_op()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver(weights.values() + biases.values())\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    # Create a session for running Ops on the Graph.\n",
    "    sess = tf.Session(\n",
    "                    config=tf.ConfigProto(\n",
    "                                    allow_soft_placement=True,\n",
    "                                    log_device_placement=True\n",
    "                                    )\n",
    "                    )\n",
    "    sess.run(init)\n",
    "    if os.path.isfile(model_filename) and use_pretrained_model:\n",
    "      saver.restore(sess, model_filename)\n",
    "\n",
    "    # Create summary writter\n",
    "    merged = tf.merge_all_summaries()\n",
    "    train_writer = tf.train.SummaryWriter('./visual_logs/train', sess.graph)\n",
    "    test_writer = tf.train.SummaryWriter('./visual_logs/test', sess.graph)\n",
    "    for step in xrange(FLAGS.max_steps):\n",
    "      start_time = time.time()\n",
    "      train_images, train_labels, _, _, _ = input_data.read_clip_and_label(\n",
    "                      filename='list/train.list',\n",
    "                      batch_size=FLAGS.batch_size * gpu_num,\n",
    "                      num_frames_per_clip=c3d_model.NUM_FRAMES_PER_CLIP,\n",
    "                      crop_size=c3d_model.CROP_SIZE,\n",
    "                      shuffle=True\n",
    "                      )\n",
    "      sess.run(train_op, feed_dict={\n",
    "                      images_placeholder: train_images,\n",
    "                      labels_placeholder: train_labels\n",
    "                      })\n",
    "      duration = time.time() - start_time\n",
    "      print('Step %d: %.3f sec' % (step, duration))\n",
    "\n",
    "      # Save a checkpoint and evaluate the model periodically.\n",
    "      if (step) % 10 == 0 or (step + 1) == FLAGS.max_steps:\n",
    "        saver.save(sess, os.path.join(model_save_dir, 'c3d_ucf_model'), global_step=step)\n",
    "        print('Training Data Eval:')\n",
    "        summary, acc = sess.run(\n",
    "                        [merged, accuracy],\n",
    "                        feed_dict={\n",
    "                                        images_placeholder: train_images,\n",
    "                                        labels_placeholder: train_labels\n",
    "                                        })\n",
    "        print (\"accuracy: \" + \"{:.5f}\".format(acc))\n",
    "        train_writer.add_summary(summary, step)\n",
    "        print('Validation Data Eval:')\n",
    "        val_images, val_labels, _, _, _ = input_data.read_clip_and_label(\n",
    "                        filename='list/test.list',\n",
    "                        batch_size=FLAGS.batch_size * gpu_num,\n",
    "                        num_frames_per_clip=c3d_model.NUM_FRAMES_PER_CLIP,\n",
    "                        crop_size=c3d_model.CROP_SIZE,\n",
    "                        shuffle=True\n",
    "                        )\n",
    "        summary, acc = sess.run(\n",
    "                        [merged, accuracy],\n",
    "                        feed_dict={\n",
    "                                        images_placeholder: val_images,\n",
    "                                        labels_placeholder: val_labels\n",
    "                                        })\n",
    "        print (\"accuracy: \" + \"{:.5f}\".format(acc))\n",
    "        test_writer.add_summary(summary, step)\n",
    "  print(\"done\")\n",
    "\n",
    "def main(_):\n",
    "  run_training()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowroot]",
   "language": "python",
   "name": "conda-env-tensorflowroot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
